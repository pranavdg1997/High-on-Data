{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Tweet classification.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imith6DlFUBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing basic libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "#Importing pytorch functions and modules\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.get_device_name(0))\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torchtext import data\n",
        "from sklearn.metrics import *\n",
        "\n",
        "#Import NLP libraries\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('popular')\n",
        "\n",
        "#Setting random seed for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCdmwTA1FUBG",
        "colab_type": "text"
      },
      "source": [
        "### Functions for text processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz55KDHNFUBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|,;]^a-zA-Z#')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "        \n",
        "    return input_txt    \n",
        "  \n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
        "                    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
        "                    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n",
        "                    \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "                    \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
        "                    \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \n",
        "                    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
        "                    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
        "                    \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\",\n",
        "                    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
        "                    \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n",
        "                    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
        "                    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
        "                    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n",
        "                    \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n",
        "                    \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
        "                    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                    \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n",
        "                    \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# Usage\n",
        "replace_contractions(\"this's a text with contraction\")\n",
        "    \n",
        "\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified, pre-processed tokens\n",
        "    \"\"\"\n",
        "    text = ' '.join(text.split())\n",
        "    #print(text)\n",
        "    text = text.lower() # lowercase text\n",
        "    #print(text)\n",
        "    text = ' '.join(word for word in text.split() if not(word.startswith('@')))\n",
        "    text = ' '.join(word for word in text.split() if not(word.startswith('http')))\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    #print(text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "\n",
        "    text = ' '.join([word for word in text.split() if len(word)> 3]) # delete short words\n",
        "    text = ' '.join([word.replace(\"#\",'') for word in text.split() if len(word)> 3])\n",
        "    text = replace_contractions(text)\n",
        "    if(len(text.split())==0):\n",
        "      text = \"blank\"\n",
        "\n",
        "    return text.split()\n",
        "  \n",
        "def to_float_array(str):\n",
        "  str = ' '.join(str.split())\n",
        "  str = str.replace(\"[\",\"\")\n",
        "  str = str.replace(\"]\",\"\")\n",
        "  str = np.array(str.split()).astype(np.float)\n",
        "  return(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlCuk93fFUBJ",
        "colab_type": "code",
        "outputId": "5cf3edaa-da2a-42d5-a010-c3cdf0f90af1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "labelled_tweets = pd.read_csv(\"labelled_tweets_twint.csv\")[[\"id\",\"drug_user\",\"tweet\"]].dropna()\n",
        "labelled_tweets.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>drug_user</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.172183e+18</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Again? Again? Codeine no dey quick clear for b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.172175e+18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>At least he doesn’t play Codeine, though.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.171842e+18</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Yeah, have you tried oxy tho? Or Vicodin? Or t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.170154e+18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>At the store currently, will be doing an IRL s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.172236e+18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>They are also the organizers of an upcoming ev...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             id  drug_user                                              tweet\n",
              "0  1.172183e+18        1.0  Again? Again? Codeine no dey quick clear for b...\n",
              "1  1.172175e+18        0.0          At least he doesn’t play Codeine, though.\n",
              "2  1.171842e+18        1.0  Yeah, have you tried oxy tho? Or Vicodin? Or t...\n",
              "3  1.170154e+18        0.0  At the store currently, will be doing an IRL s...\n",
              "4  1.172236e+18        0.0  They are also the organizers of an upcoming ev..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Prpqt1DFUBN",
        "colab_type": "code",
        "outputId": "c4596f7a-2e4a-4a91-cce5-e6961dfc8e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "labelled_tweets.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 493 entries, 0 to 4007\n",
            "Data columns (total 3 columns):\n",
            "id           493 non-null float64\n",
            "drug_user    493 non-null float64\n",
            "tweet        493 non-null object\n",
            "dtypes: float64(2), object(1)\n",
            "memory usage: 15.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yt7D3TzFUBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data,val_data,_,_ = train_test_split(labelled_tweets,labelled_tweets[\"drug_user\"],\n",
        "                                           stratify=labelled_tweets[\"drug_user\"].values,test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtXGbNM8FUBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.to_csv(\"train_data.csv\",index=False)\n",
        "val_data.to_csv(\"val_data.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTznJBkqFUBU",
        "colab_type": "code",
        "outputId": "0355eec5-62be-4420-90c9-f05660edea7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "TWEET = data.Field(sequential=True, tokenize=custom_tokenizer, lower=True,include_lengths = True)\n",
        "LABEL = data.LabelField(sequential=False, use_vocab=True,pad_token=None,unk_token=None)\n",
        "\n",
        "\n",
        "fields = [(None, None), ('l', LABEL), ('t', TWEET)]\n",
        "\n",
        "train_data, valid_data = data.TabularDataset.splits(\n",
        "                                        path = '',\n",
        "                                        train = 'train_data.csv',\n",
        "                                        validation = 'val_data.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True\n",
        ")\n",
        "\n",
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TWEET.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "print(\"Unique tokens in TWEET vocabulary: \",len(TWEET.vocab))\n",
        "print(\"Unique tokens in LABEL vocabulary: \",len(LABEL.vocab))\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key=lambda x:len(x.t),\n",
        "    sort=True,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TWEET vocabulary:  2064\n",
            "Unique tokens in LABEL vocabulary:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5mYHHpYFUBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BI_LSTM_VANILLA(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)#,enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))       \n",
        "        return torch.sigmoid(self.fc(hidden.squeeze()))\n",
        "      \n",
        "\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    y_true = y.detach().cpu()\n",
        "    y_pred = torch.round(preds).detach().cpu()\n",
        "    acc = accuracy_score(y_true,y_pred)\n",
        "    return acc\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    #print(0)\n",
        "    model.train()\n",
        "    #print(1)\n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.t\n",
        "        #print(2)\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        \n",
        "        loss = criterion(predictions.type(torch.float), batch.l.type(torch.float))\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.l)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    #print(\"training done\")    \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.t\n",
        "\n",
        "            \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions.type(torch.float), batch.l.type(torch.float))\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.l)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    #print(\"evaluation done\")\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "  \n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WYADYOWFUBZ",
        "colab_type": "code",
        "outputId": "185a8d99-cddf-485c-b3dd-e38b1260aa8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "INPUT_DIM = len(TWEET.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "LR = 0.0001\n",
        "PAD_IDX = TWEET.vocab.stoi[TWEET.pad_token]\n",
        "\n",
        "model = BI_LSTM_VANILLA(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX).to(device)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "pretrained_embeddings = TWEET.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "UNK_IDX = TWEET.vocab.stoi[TWEET.unk_token]\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=LR)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\n",
        "N_EPOCHS = 200\n",
        "EARLY_STOPPING_ROUNDS = 2\n",
        "curr_rounds = 0\n",
        "best_valid_acc = (-1)*float('inf')\n",
        "epoch = 0\n",
        "overfitting = False\n",
        "\n",
        "while((epoch < N_EPOCHS)&(overfitting==False)):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    if(valid_acc > best_valid_acc):\n",
        "        best_valid_acc = valid_acc\n",
        "        curr_rounds = 0\n",
        "    else:\n",
        "        curr_rounds +=1\n",
        "        if(curr_rounds==EARLY_STOPPING_ROUNDS):\n",
        "            overfitting=True\n",
        "            print(\"Overfitting, stopping after this epoch\")\n",
        "\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    \n",
        "    if(epoch%10==0):\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 837,441 trainable parameters\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.688 | Train Acc: 59.17%\n",
            "\t Val. Loss: 0.689 |  Val. Acc: 62.90%\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.689 | Train Acc: 58.62%\n",
            "\t Val. Loss: 0.687 |  Val. Acc: 63.71%\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.684 | Train Acc: 62.32%\n",
            "\t Val. Loss: 0.685 |  Val. Acc: 63.71%\n",
            "Overfitting, stopping after this epoch\n",
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.684 | Train Acc: 62.54%\n",
            "\t Val. Loss: 0.682 |  Val. Acc: 63.71%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC0qBEIYFUBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok for tok in custom_tokenizer(sentence)]\n",
        "    indexed = [TWEET.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = model(tensor, length_tensor)\n",
        "    prob = prediction.item()\n",
        "    return(round(prob,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kexrYzpKFUBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scraped_tweets = pd.read_csv(\"scraped_tweets_twint.csv\")\n",
        "scraped_tweets[\"drug_use_probability\"] = scraped_tweets[\"tweet\"].apply(lambda x:predict_sentiment(model,x))\n",
        "scraped_tweets.to_csv(\"drug_use_predictions.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF0Bsb7vFUBg",
        "colab_type": "code",
        "outputId": "21993c8a-4345-420e-d1e9-478fcf326594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "plt.figure()\n",
        "sns.distplot(scraped_tweets[\"drug_use_probability\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdf7bad0fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEHCAYAAAC3Ph1GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncGOM8ShL4MYY41GWwI0xxqP+PzvSDV23Wpo/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA5wTsPfK7I5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}